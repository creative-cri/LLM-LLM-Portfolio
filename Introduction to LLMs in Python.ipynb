{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32c0cab",
   "metadata": {},
   "source": [
    "# Introduction to LLMs in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c826e8",
   "metadata": {},
   "source": [
    "# 1. The Large Language Models (LLMs) Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ea24d",
   "metadata": {},
   "source": [
    "## Introducing large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23216a7",
   "metadata": {},
   "source": [
    "### Classifying a restaurant's customer review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad10a4",
   "metadata": {},
   "source": [
    "Let's practice loading an LLM from the Hugging Face hub into a pipeline to perform sentiment classification of customer restaurant reviews.\n",
    "\n",
    "Specifying the target language task when calling the pipeline() function is enough often to load a \"default\" model from Hugging Face. Nonetheless, it is usually a good practice to specify the name of the model we want to use. This is done by adding the model argument to the pipeline() function.\n",
    "\n",
    "The model_name variable, has been already instantiated for you with the name of a BERT-based LLM particularly suited for classifying reviews in a 1-to-5 star rating scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for loading Hugging Face pipelines\n",
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
    "\n",
    "# Load the pipeline for sentiment classification\n",
    "classifier = pipeline(\"text-classification\", model=model_name)\n",
    "\n",
    "# Pass the customer review to the model for prediction\n",
    "prediction = classifier(prompt)\n",
    "print(prediction)\n",
    "\n",
    "\"\"\"\n",
    "[{'label': '3 stars', 'score': 0.6387939453125}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327fbca8",
   "metadata": {},
   "source": [
    "## Tasks LLMs can perform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4fa01",
   "metadata": {},
   "source": [
    "### Using a pipeline for summarization\n",
    "\n",
    "In this exercise, you'll practice loading a Hugging Face LLM into a pipeline for text summarization. This is a remarkable but challenging language task that requires sequence-to-sequence LLMs -such as T5 models- to output a summarized sequence given an original text sequence.\n",
    "\n",
    "The pipeline import has been made for you. The text to be summarized has also been defined in the long_text variable. The beginning of the text looks like this:\n",
    "\n",
    "The tower is 324 meters (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b743649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pipeline for text summarization\n",
    "summarizer = pipeline('summarization', model=model_name)\n",
    "\n",
    "# Pass the long text to the model to summarize it\n",
    "outputs = summarizer(long_text, max_length=50)\n",
    "\n",
    "# Access and print the summarized text in the outputs variable\n",
    "print(outputs[0]['summary_text'])\n",
    "\n",
    "\"\"\"\n",
    " the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in \n",
    " Paris. Its base is square, measuring 125 metres\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045cbe5",
   "metadata": {},
   "source": [
    "### Time for some question-answering!\n",
    "\n",
    "Next, let's practice loading a Hugging Face LLM into a pipeline for question-answering (QA, for short). This time, you will use the default model supplied by Hugging Face transformers library for QA pipelines.\n",
    "\n",
    "The necessary imports have been made for you, along with specifying the following text in a context variable (the first half of the text is shown below):\n",
    "\n",
    "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 12.5 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York was finished in 1930…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2589917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pipeline for question-answering\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
    "\n",
    "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
    "outputs = qa_model(question=question, context=context)\n",
    "\n",
    "# Access and print the answer\n",
    "print(outputs['answer'])\n",
    "\n",
    "\"\"\"\n",
    "41 years\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f873d6",
   "metadata": {},
   "source": [
    "## The transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fddfa4",
   "metadata": {},
   "source": [
    "### Hello PyTorch transformer\n",
    "\n",
    "PyTorch's nn.Transformer class provides a full transformer architecture with pre-built encoder and decoder stacks.\n",
    "\n",
    "The simplest way to manually create a skeleton nn.Transformer model is by specifying its main structural hyperparameters: model dimensionality (embedding size), number of attention heads, number of encoder layers, and number of decoder layers. PyTorch does the rest of the job for you, assigning default modules inside the encoder and decoder layers.\n",
    "\n",
    "torch.nn has been already imported for you.\n",
    "\n",
    "Note: take a deep look at the print(model) output for a very insightful glance inside the transformer model built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48741af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transformer model hyperparameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "# Create the transformer model and assign hyperparameters\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "\"\"\"\n",
    "    Transformer(\n",
    "      (encoder): TransformerEncoder(\n",
    "        (layers): ModuleList(\n",
    "          (0-5): 6 x TransformerEncoderLayer(\n",
    "            (self_attn): MultiheadAttention(\n",
    "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
    "            )\n",
    "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "            (dropout1): Dropout(p=0.1, inplace=False)\n",
    "            (dropout2): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "      )\n",
    "      (decoder): TransformerDecoder(\n",
    "        (layers): ModuleList(\n",
    "          (0-5): 6 x TransformerDecoderLayer(\n",
    "            (self_attn): MultiheadAttention(\n",
    "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
    "            )\n",
    "            (multihead_attn): MultiheadAttention(\n",
    "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
    "            )\n",
    "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "            (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "            (dropout1): Dropout(p=0.1, inplace=False)\n",
    "            (dropout2): Dropout(p=0.1, inplace=False)\n",
    "            (dropout3): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "      )\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6aeb6f",
   "metadata": {},
   "source": [
    "### Hands-on translation pipeline\n",
    "\n",
    "Before delving fully into transformers in the next chapter, let's circle back to explore a few more types of language task pipelines!\n",
    "\n",
    "In this exercise, you'll load a Hugging Face LLM into a pipeline for Spanish-to-English translation. The model, pre-specified in a variable model_name, is set as \"Helsinki-NLP/opus-mt-es-en\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e995bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Este curso sobre LLMs se está poniendo muy interesante\"\n",
    "\n",
    "# Define pipeline for Spanish-to-English translation\n",
    "translator = pipeline(\"translation_es_to_en\", model=model_name)\n",
    "\n",
    "# Translate the input text\n",
    "translations = translator(input_text, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Access the output to print the translated text in English\n",
    "print(translations[0]['translation_text'])\n",
    "\n",
    "\"\"\"\n",
    "This course on LLMs is getting very interesting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f590c7",
   "metadata": {},
   "source": [
    "### Generating replies to customer reviews\n",
    "\n",
    "In this exercise, you'll practice using an LLM pipeline for text generation.\n",
    "\n",
    "A text variable has been defined, containing a customer review for Riverview Hotel:\n",
    "\n",
    "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had\n",
    "\n",
    "The language task consists in generating a hotel reply to the customer review. The initial sentence for the reply is defined in the response variable so that the LLM gets it prompted along with the customer review to continue generating the reply.\n",
    "\n",
    "Note: the pad_token_id=generator.tokenizer.eos_token_id argument sets the tokenizer padding token ID as the EOS (End Of Speech) token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n",
    "# Build the prompt for the text generation LLM\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "# Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])\n",
    "\n",
    "\"\"\"\n",
    "    Customer review:\n",
    "    I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. \n",
    "    The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
    "    \n",
    "    Hotel reponse to the customer:\n",
    "    Dear valued customer, I am glad to hear you had a good stay with us. Please allow us to have a moment to chat about the \n",
    "    experience and thank you for the fantastic service you gave us. We are pleased to provide you with the \"A\" in all the \n",
    "    pages alleged to be there because it is your information and not your personal information!! Thank you for your \n",
    "    understanding. Best wishes & Thanks!\n",
    "    \n",
    "    Customer review:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987b943",
   "metadata": {},
   "source": [
    "# 2. Building a Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c25b9",
   "metadata": {},
   "source": [
    "## Attention mechanisms and positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8db578",
   "metadata": {},
   "source": [
    "### Hands-on positional encoding\n",
    "\n",
    "In this exercise you'll complete the class implementation for a positional encoding mechanism.\n",
    "\n",
    "The necessary imports have been done for you, namely import torch.nn as nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass an appropriate PyTorch class \n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize the positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate and assign position encodings to the matrix\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    # Update the embeddings tensor adding the positional encodings\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f25cea",
   "metadata": {},
   "source": [
    "### Implementing multi-headed self-attention\n",
    "\n",
    "Now it's the turn of the multi-headed self-attention mechanism implementation.\n",
    "\n",
    "Besides the necessary imports, including this time torch.nn.functional as F, the __init__() method is also provided.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)      \n",
    "        self.output_linear = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(self, x, batch_size):\n",
    "    # Split the sequence embeddings in x across the attention heads\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "    return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(self, query, key, mask=None):\n",
    "    # Compute dot-product attention scores\n",
    "    scores = torch.matmul(query, key.permute(1, 2, 0))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "    # Normalize attention scores into attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34019626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, query, key, value, mask=None):\n",
    "    batch_size = query.size(0)\n",
    "\n",
    "    query = self.split_heads(self.query_linear(query), batch_size)\n",
    "    key = self.split_heads(self.key_linear(key), batch_size)\n",
    "    value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "    attention_weights = self.compute_attention(query, key, mask)\n",
    "\t\t\n",
    "    # Multiply attention weights by values, concatenate and linearly project outputs\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "    return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85ffe8",
   "metadata": {},
   "source": [
    "## Building an encoder transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c38b2",
   "metadata": {},
   "source": [
    "### Post-attention feed-forward layer\n",
    "\n",
    "Let's assemble some of the pieces of an encoder transformer, starting with the feed-forward sublayer that follows multi-headed self-attention in every encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\t# Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525f49e",
   "metadata": {},
   "source": [
    "### Time for an encoder layer\n",
    "\n",
    "You've made it quite far in building your own skeleton transformer architecture! Now you are ready to assemble a full encoder layer containing:\n",
    "\n",
    "A multi-headed self-attention mechanism.\n",
    "A feed-forward sublayer.\n",
    "A combined layer normalization and dropout to be applied after each of the above two stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6728c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a474b",
   "metadata": {},
   "source": [
    "### Encoder transformer body and head\n",
    "\n",
    "Almost there! Now that the encoder layer implementation has been completed, all that remains is:\n",
    "\n",
    "Implementing the transformer body, namely a stack of multiple encoder layers.\n",
    "Appending a task-specific transformer head to process the encoder's resulting hidden states and produce the final outputs for the language task at hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99310786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\t\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2179e39",
   "metadata": {},
   "source": [
    "### Testing the encoder transformer\n",
    "\n",
    "In this exercise, you'll practice creating some instructions to pass an example random sequence throughout the encoder transformer you just defined to obtain and print the classification output. The following variables and model hyperparameters are defined for you:\n",
    "\n",
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 256\n",
    "dropout = 0.1\n",
    "The PositionalEncoder, MultiHeadAttention, FeedForwardSublayer,EncoderLayer, TransformerEncoder, and ClassifierHead classes are also implemented.\n",
    "\n",
    "Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Complete the forward pass \n",
    "output = encoder(input_sequence, mask)\n",
    "classification = classifier(output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification)\n",
    "\n",
    "\"\"\"\n",
    "   Classification outputs for a batch of  8 sequences:\n",
    "    tensor([[ 0.3724,  0.0636,  0.5129],\n",
    "            [-0.1837,  0.5669, -0.9256],\n",
    "            [-0.1848, -0.2706,  0.1537],\n",
    "            [ 0.0478,  0.2004, -0.2376],\n",
    "            [ 0.6299,  0.4149,  0.2964],\n",
    "            [ 1.3734, -0.0549, -0.0309],\n",
    "            [-0.0408,  0.3052, -0.1994],\n",
    "            [ 0.5111,  0.5409,  0.2535]], grad_fn=<AddmmBackward0>)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e4d68",
   "metadata": {},
   "source": [
    "## Building a decoder transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2788ea0",
   "metadata": {},
   "source": [
    "### Building a decoder body and head\n",
    "\n",
    "Time to design a high-level architecture for a decoder-only transformer! On this occasion, instead of building the model body and the model head in two separate classes, the model head will be incorporated as part of the model body class that contains the stack of decoder layers.\n",
    "\n",
    "As usual, the necessary imports for this exercise have been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f370d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad083c",
   "metadata": {},
   "source": [
    "### Testing the decoder transformer\n",
    "\n",
    "In this exercise, you'll practice creating some instructions to pass an example random sequence throughout a decoder transformer architecture to obtain outputs in the form of next-token probabilities across the vocabulary.\n",
    "\n",
    "The following variables and model hyperparameters are defined for you:\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "num_layers = 6\n",
    "\n",
    "d_ff = 2048\n",
    "\n",
    "sequence_length = 256\n",
    "\n",
    "dropout = 0.1\n",
    "\n",
    "The PositionalEncoder, MultiHeadAttention, PositionWiseFeedForward,DecoderLayer, and TransformerDecoder classes are also implemented, the last of which integrates the model body and head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374791df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
    "\n",
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    torch.Size([8, 256, 10000])\n",
    "    tensor([[[ -9.8297,  -9.5104,  -9.7126,  ...,  -9.5520,  -9.1028,  -8.9959],\n",
    "             [ -9.8432,  -8.9471, -10.0882,  ...,  -8.7613,  -8.4118, -10.0511],\n",
    "             [ -9.8982,  -9.3050, -10.3777,  ...,  -8.9013,  -8.9952,  -9.3154],\n",
    "             ...,\n",
    "             [ -9.7825,  -9.3963,  -9.3710,  ...,  -9.5593,  -9.4124,  -8.8603],\n",
    "             [ -9.4594,  -9.5378,  -9.3810,  ...,  -8.3339,  -9.9532,  -8.9075],\n",
    "             [ -9.6926,  -9.4519,  -9.2744,  ...,  -9.1344,  -8.9179,  -8.9821]],\n",
    "    \n",
    "            [[ -9.8243, -10.0416,  -9.0161,  ...,  -9.6624,  -9.9786,  -8.3693],\n",
    "             [ -8.8322,  -9.3262,  -8.6158,  ...,  -8.4551, -10.2405,  -9.0333],\n",
    "             [ -9.7660,  -9.0521,  -8.3632,  ...,  -9.8095,  -9.8669,  -8.3530],\n",
    "             ...,\n",
    "             [-10.5909,  -9.5141,  -9.0186,  ...,  -8.9326,  -9.4526, -10.1048],\n",
    "             [-10.8859,  -8.9201, -10.1515,  ...,  -8.9360,  -8.7401,  -8.7596],\n",
    "             [-10.7924,  -8.9120,  -9.6745,  ...,  -9.7435, -10.4142,  -9.0034]],\n",
    "    \n",
    "            [[ -9.1292,  -9.1546,  -8.9989,  ...,  -9.2599,  -8.6355, -10.4262],\n",
    "             [ -8.8719,  -8.6458,  -9.4428,  ...,  -8.3387,  -8.8085,  -8.8325],\n",
    "             [-10.3137,  -9.3272,  -9.0327,  ..., -10.1166,  -8.6670, -10.1126],\n",
    "             ...,\n",
    "             [ -8.6443,  -9.9641,  -8.6314,  ...,  -8.2978,  -9.2239,  -9.5556],\n",
    "             [ -9.9164, -10.2271,  -9.8148,  ...,  -9.5189,  -9.8326,  -9.2970],\n",
    "             [ -9.3705,  -9.1985,  -9.2066,  ...,  -9.6071, -10.2994,  -8.4551]],\n",
    "    \n",
    "            ...,\n",
    "    \n",
    "            [[ -9.3215,  -9.3681,  -9.4940,  ...,  -9.3820,  -8.3590, -10.1993],\n",
    "             [ -9.3941,  -9.5416,  -9.0704,  ..., -10.1786,  -9.8988,  -9.5838],\n",
    "             [ -9.2433,  -9.1029,  -9.5273,  ...,  -9.0178,  -7.6837,  -9.2063],\n",
    "             ...,\n",
    "             [ -9.5017,  -8.6286,  -9.4111,  ...,  -9.3987,  -9.1002,  -8.9501],\n",
    "             [ -9.5125,  -8.7731,  -9.0562,  ...,  -9.5856,  -9.4330,  -8.8783],\n",
    "             [ -9.1607,  -9.4391,  -9.4114,  ...,  -7.7130, -10.4272,  -9.9330]],\n",
    "    \n",
    "            [[ -9.6414,  -9.2776,  -9.9829,  ...,  -9.0220,  -9.5565,  -9.0914],\n",
    "             [ -9.4192,  -9.1742,  -9.8931,  ...,  -9.7211,  -9.2576,  -9.3236],\n",
    "             [ -8.9370,  -9.1491,  -9.1411,  ...,  -8.4854, -10.0966,  -8.8847],\n",
    "             ...,\n",
    "             [ -9.5567,  -8.5548,  -9.4768,  ...,  -9.9121,  -8.5085,  -9.1623],\n",
    "             [ -9.6652,  -9.3781,  -9.3925,  ...,  -8.8631,  -8.3149,  -9.1920],\n",
    "             [ -9.1550,  -9.5085,  -9.1636,  ...,  -9.2894,  -9.2322,  -8.9062]],\n",
    "    \n",
    "            [[ -9.0840,  -8.9695,  -8.9216,  ...,  -9.7993, -10.1810,  -9.5698],\n",
    "             [-10.0745,  -8.8612,  -9.1784,  ..., -10.1985,  -9.1861,  -9.3601],\n",
    "             [ -9.1717,  -9.0767,  -9.0042,  ...,  -9.7493,  -8.7475,  -9.3801],\n",
    "             ...,\n",
    "             [ -9.9172,  -9.4276,  -9.6445,  ...,  -8.6570,  -9.0422, -10.1084],\n",
    "             [ -9.8328,  -9.4414,  -8.4021,  ...,  -9.3645,  -8.8457,  -8.9731],\n",
    "             [ -9.7629,  -9.6181,  -8.8173,  ...,  -8.8028,  -9.4497,  -8.3002]]],\n",
    "           grad_fn=<LogSoftmaxBackward0>)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4a270",
   "metadata": {},
   "source": [
    "## Building an encoder-decoder transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37793162",
   "metadata": {},
   "source": [
    "### Incorporating cross-attention in a decoder\n",
    "\n",
    "In an encoder-decoder transformer, decoder layers incorporate two attention mechanisms: the causal attention inherent to any transformer decoder, plus a cross-attention that integrates source sequence information processed by the encoder with the target sequence information being processed through the decoder.\n",
    "\n",
    "In this exercise you'll modify the DecoderLayer class to incorporate this twofold attention scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a644734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc43ed",
   "metadata": {},
   "source": [
    "### Trying out an encoder-decoder transformer\n",
    "\n",
    "Your next task is complete the following piece of code to define and forward-pass an example batch of randomly generated input sequences through an encoder-decoder transformer.\n",
    "\n",
    "Remember that we are only testing a yet-to-be-trained transformer architecture, hence the use of random input sequences.\n",
    "\n",
    "These are the model hyperparameters and variables used:\n",
    "\n",
    "vocab_size = 10000\n",
    "batch_size = 16\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1\n",
    "The example assumes the necessary imports and the following transformer architecture classes have been defined for you: MultiHeadAttention, FeedForwardSubLayer, PositionalEncoding, EncoderLayer, DecoderLayer, TransformerEncoder, TransformerDecoder, and ClassifierHead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)\n",
    "\n",
    "\"\"\"\n",
    "Batch's output shape:  torch.Size([16, 64, 512])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb73aef",
   "metadata": {},
   "source": [
    "### Transformer assembly bottom-up\n",
    "\n",
    "This exercise focuses on putting together the main building blocks of an encoder-only transformer architecture, using a bottom-up approach.\n",
    "\n",
    "The following classes, their attributes, and their core functions have been defined for you:\n",
    "\n",
    "PositionalEncoding(nn.Module): positional encoding for input embeddings.\n",
    "MultiHeadAttention(nn.Module): multi-head attention layer.\n",
    "FeedForward(nn.Module): feed-forward layer.\n",
    "EncoderLayer(nn.Module): a replicable encoder layer that glues together multi-head attention and feed-forward layers, along with layer normalizations and dropouts.\n",
    "Your next task is to finalize assembling the highest-level components of the encoder transformer: the TransformerEncoder and Transformer classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c0c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize positional encoding layer and stack of EncoderLayer modules\n",
    "class TransformerEncoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the sequence through each layer in the encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Initialize the encoder stack of the Transformer\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cfa19",
   "metadata": {},
   "source": [
    "# 3. Harnessing Pre-trained LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf20630",
   "metadata": {},
   "source": [
    "## LLMs for text classification and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50eadb2",
   "metadata": {},
   "source": [
    "### Classifying two movie opinions\n",
    "\n",
    "We have seen how to pass one example sequence to a pre-trained text classification LLM for inference. In this exercise you will practice passing two example sequences simultaneously, describing two rather opposite opinions of a movie.\n",
    "\n",
    "All the necessary imports have been made for you, including the auto classes specific to using pre-trained classification LLMs. The variable model_name has been also set with the name of the BERT-based model to use: \"textattack/distilbert-base-uncased-SST-2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec927a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  model_name, num_labels=2)\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")\n",
    "    \n",
    "\"\"\"\n",
    "    Predicted class for \"The best movie I've ever watched!\": 1\n",
    "    Predicted class for \"What an awful movie. I regret watching it.\": 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd2cfa",
   "metadata": {},
   "source": [
    "## LLMs for text summarization and translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca0a37",
   "metadata": {},
   "source": [
    "### Summarizing a product opinion\n",
    "\n",
    "In this text summarization exercise, we will examine different aspects of the \"opinosis\" dataset containing product reviews and summaries, as well as showing an example input sequence and its generated summarization.\n",
    "\n",
    "The necessary imports have been made for you, including the AutoTokenizer class and the specific auto class for handling sequence-to-sequence models: AutoModelForSeq2SeqLM.\n",
    "\n",
    "Furthermore, these instructions have been executed a priori to load the dataset, tokenizer, and pre-trained model:\n",
    "\n",
    "dataset = load_dataset(\"opinosis\")\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of instances: {len(dataset['train'])}\")\n",
    "\n",
    "# Show the names of features in the training fold of the dataset\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")\n",
    "\n",
    "# Encode the input example, obtain the summary, and decode it\n",
    "example = dataset['train'][-2]['review_sents']\n",
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = model.generate(input_ids, max_length=150)\n",
    "summary = tokenizer.decode(\n",
    "  summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)\n",
    "\n",
    "\"\"\"\n",
    "    Number of instances: 51\n",
    "    Feature names: ['review_sents', 'summaries']\n",
    "    \n",
    "    Original Text (first 400 characters): \n",
    "     I bought the 8, gig Ipod Nano that has the built, in video camera .\n",
    "      Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod .\n",
    "    I have lots of music cd's and dvd's, so currently I'm just interested in storing some of my music and videos on the ipod so\n",
    "    I can enjoy them on my vacation, and while at work .\n",
    "    There's a right way and wrong wa\n",
    "    \n",
    "    Generated Summary: \n",
    "     I bought the 8, gig Ipod Nano that has the built, in video camera. Itunes has an on, line store, where you may purchase \n",
    "     and download music and videos which will install onto the ipod.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b873b1b",
   "metadata": {},
   "source": [
    "### The Spanish phrasebook mission\n",
    "\n",
    "You are a content writer at a reputable travel guide publisher. The next title to be published is a Spain travel guide for English speakers, but due to high demand and limited human resources, they assigned you the urgent task of drafting a \"Spanish phrasebook\" page, covering some essential survival Spanish words and phrases.\n",
    "\n",
    "Luckily, LLMs are here to help! In this exercise, you'll try using a pre-trained LLM for English-to-Spanish translation, and start this important mission by translating the first five common English phrases into Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b350d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "for english_input in english_inputs:\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors='pt')\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")\n",
    "    \n",
    "\"\"\"\n",
    "    English: Hello | Spanish: Hola.\n",
    "    English: Thank you | Spanish: Gracias.\n",
    "    English: How are you? | Spanish: ¿Cómo estás?\n",
    "    English: Sorry | Spanish: Lo siento.\n",
    "    English: Goodbye | Spanish: Adiós.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265ef48",
   "metadata": {},
   "source": [
    "## LLMs for question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928268a1",
   "metadata": {},
   "source": [
    "### Load and inspect a QA dataset\n",
    "\n",
    "In this exercise, you will load a dataset for extractive QA, inspect some data, and tokenize a question-context example into a suitable format for feeding it to an LLM for QA.\n",
    "\n",
    "The necessary libraries, classes, and functions have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d80c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific subset of the dataset \n",
    "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
    "\n",
    "question = mlqa[\"test\"][\"question\"][0]\n",
    "context = mlqa[\"test\"][\"context\"][0]\n",
    "print(\"Question: \", question)\n",
    "print(\"Context: \", context)\n",
    "\n",
    "# Initialize the tokenizer using the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepset/minilm-uncased-squad2')\n",
    "\n",
    "# Tokenize the inputs returning the result as tensors\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "print(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])\n",
    "\n",
    "\"\"\"\n",
    " Question:  Who analyzed the biopsies?\n",
    " \n",
    " Context:  In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF\n",
    " and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington \n",
    " University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been \n",
    " burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists,\n",
    " who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had \n",
    " sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of \n",
    " Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled \n",
    " toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs\n",
    " handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly \n",
    " exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the \n",
    " House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area \n",
    " 51 in order to protect themselves from a lawsuit.\"\n",
    " \n",
    " First five encoded tokens:  tensor([  101,  2040, 16578,  1996, 16012])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fc966",
   "metadata": {},
   "source": [
    "### Extract and decode the answer \n",
    "\n",
    "Time to practice using a fine-tuned LLM to extract the answer to a question!\n",
    "\n",
    "The necessary imports and steps before initializing the LLM have been done for you, including loading an example question and context from the xtreme dataset, defining the model checkpoint in model_ckp, passing the question and context to the tokenizer, and storing the result in inputs.\n",
    "\n",
    "This is the example question: How many bodies can sit in Stade Vellodrome?\n",
    "\n",
    "A passage from its context: […]The club had a history of success under then-owner Bernard Tapie. The club's home, the Stade Vélodrome, which can seat around 67,000 people, also functions for other local sports, as well as the national rugby team.[…]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM upon the model checkpoint and forward-pass the input\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM upon the model checkpoint\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Forward-pass the input through the model\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "# Get the most likely start and end answer position from the raw LLM outputs\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Access the tokenized inputs tensor to get the answer span\n",
    "answer_span = inputs['input_ids'][0][start_idx:end_idx]\n",
    "\n",
    "# Decode the answer span to get the extracted answer text\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "\"\"\"\n",
    "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering:\n",
    "['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
    "\n",
    "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or \n",
    "with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "\n",
    "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be \n",
    "exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "\n",
    "\n",
    "\n",
    "<script.py> output:\n",
    "    Answer:  67, 000 people\n",
    "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: \n",
    "['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
    "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or \n",
    "with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be \n",
    "exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47395cc2",
   "metadata": {},
   "source": [
    "## LLM fine-tuning and transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee29ba",
   "metadata": {},
   "source": [
    "### Fine-tuning preparations\n",
    "\n",
    "We have seen that fine-tuning a pre-trained LLM mainly involves getting and preparing the data needed to fine-tune the model to a specific problem, as well as setting up the necessary classes and arguments before starting a (likely very time-consuming!) training loop.\n",
    "\n",
    "In this exercise, we will revisit the process of setting up a training loop before fine-tuning a model. Besides all the necessary imports, the model_name = \"distilbert-base-uncased\" variable and its associated tokenizer have been defined for you, as well as the tokenized training data in tokenized_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aeb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained LLM, specifying its use for binary classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Set up training arguments with a batch size of 8 per GPU and 5 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smaller_bert_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    ")\n",
    "# Set up trainer, assigning previously set up training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198be52b",
   "metadata": {},
   "source": [
    "### The inside-out LLM\n",
    "\n",
    "The emotions dataset is a labeled dataset containing tweets and their associated emotion label, out of six different classes: sadness, joy, love, anger, fear, and surprise.\n",
    "\n",
    "In this exercise the pre-trained \"distilbert-base-uncased\" model has been loaded for a text classification task.\n",
    "\n",
    "Your job will be to fine-tune this model for tweet emotion classification using the emotions dataset, that has been previously loaded, encoded and tokenized into emotions_encoded[\"train\"], emotions_encoded[\"validation\"], emotions_encoded[\"test\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d349695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer and assign a training and validation set to it\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "    \t\t\tcompute_metrics=compute_metrics,\n",
    "    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n",
    "    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n",
    "    \t\t\ttokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Training loop to fine-tune the model\n",
    "# trainer.train()\n",
    "\n",
    "input_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n",
    "\n",
    "# Tokenize the input sequences and pass them to the model\n",
    "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain class labels from raw predictions\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
    "\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    \n",
    "\"\"\"\n",
    "    Input Text 1: It's dark and rainy outside\n",
    "    Predicted Label: 4\n",
    "    \n",
    "    Input Text 2: I love penguins!\n",
    "    Predicted Label: 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8981b",
   "metadata": {},
   "source": [
    "# 4. Evaluating and Leveraging LLMs in the Real World"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca07ac",
   "metadata": {},
   "source": [
    "## Guidelines and standard metrics for evaluating LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b8a72",
   "metadata": {},
   "source": [
    "### Calculating accuracy\n",
    "\n",
    "In this exercise you will use a sentiment classification pipeline to classify four short reviews with known labels, and then calculate the accuracy of predictions using the evaluate library.\n",
    "\n",
    "The necessary imports have been made for you. The test_examples variable contains the text reviews and their ground-truth labels:\n",
    "\n",
    "test_examples = [\n",
    "\n",
    "    {\"text\": \"I am making a good use of this product!\", \"label\": 1},\n",
    "    \n",
    "    {\"text\": \"The service was disappointing.\", \"label\": 0},\n",
    "    \n",
    "    {\"text\": \"I learned a lot from this book.\", \"label\": 1},\n",
    "    \n",
    "    {\"text\": \"The book cover broke after two days of use.\", \"label\": 0},\n",
    "    \n",
    "]\n",
    "\n",
    "The pipeline is also defined as follows: sentiment_analysis = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the four input texts (without labels) to the pipeline\n",
    "predictions = sentiment_analysis([example['text'] for example in test_examples])\n",
    "\n",
    "true_labels = [example[\"label\"] for example in test_examples]\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
    "print(result)\n",
    "\n",
    "\"\"\"\n",
    "{'accuracy': 1.0}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5d473",
   "metadata": {},
   "source": [
    "### Beyond accuracy: describing metrics\n",
    "\n",
    "It's never a bad time to revise the definitions of some popular metrics used in classification LLMs. This exercise focuses on accessing the definitions of popular metrics found in the evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf341c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accuracy, precision, recall and F1 score metrics\n",
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load('f1')\n",
    "\n",
    "# Obtain a description of each metric\n",
    "print(accuracy.description)\n",
    "print(precision.description)\n",
    "print(recall.description)\n",
    "print(f1.description)\n",
    "\n",
    "\"\"\"\n",
    "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "     Where:\n",
    "    TP: True positive\n",
    "    TN: True negative\n",
    "    FP: False positive\n",
    "    FN: False negative\n",
    "    \n",
    "    \n",
    "    Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n",
    "    Precision = TP / (TP + FP)\n",
    "    where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
    "    \n",
    "    \n",
    "    Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\n",
    "    Recall = TP / (TP + FN)\n",
    "    Where TP is the true positives and FN is the false negatives.\n",
    "    \n",
    "    \n",
    "    The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0411d9",
   "metadata": {},
   "source": [
    "### Beyond accuracy: using metrics\n",
    "\n",
    "Let's make a combined use of the metrics we described in the previous activity, to analyze an LLM performance in classifying seven hotel reviews:\n",
    "\n",
    "test_examples = [\n",
    "    \"Fantastic hotel, exceeded expectations!\",\n",
    "    \n",
    "    \"Quiet despite central location, great stay.\",\n",
    "    \n",
    "    \"Friendly staff, welcoming atmosphere.\",\n",
    "    \n",
    "    \"Spacious, comfy room—a perfect retreat.\",\n",
    "    \n",
    "    \"Cleanliness could improve, overall decent stay.\",\n",
    "    \n",
    "    \"Disappointing stay, noisy and unclean room.\",\n",
    "    \n",
    "    \"Terrible service, unfriendly staff, won't return.\"\n",
    "]\n",
    "\n",
    "The ground-truth labels associated with the above reviews are contained in this list:\n",
    "\n",
    "test_labels = [1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Pass the examples to the pipeline, and obtain a list predicted labels\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "predictions = sentiment_analysis([example for example in test_examples])\n",
    "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
    "\n",
    "# Compute the metrics by comparing real and predicted labels\n",
    "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(recall.compute(references=test_labels, predictions=predicted_labels))\n",
    "print(f1.compute(references=test_labels, predictions=predicted_labels))\n",
    "\n",
    "\"\"\"\n",
    "    {'precision': 0.8}\n",
    "    {'recall': 1.0}\n",
    "    {'f1': 0.888888888888889}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfed10",
   "metadata": {},
   "source": [
    "## Specialized metrics for language tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169788f",
   "metadata": {},
   "source": [
    "### Perplexed about 2030\n",
    "\n",
    "This exercise gives you the chance to generate some text and calculate its perplexity, based on the following prompt:\n",
    "\n",
    "prompt = \"Current trends show that by 2030 \"\n",
    "In addition to the needed imports, an AutoModelForCausalLM model instance and its tokenizer have been set upon the \"gpt2\" model, in the model and tokenizer variables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the prompt, generate text and decode it\n",
    "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(prompt_ids, max_length=20)\n",
    "generated_text = tokenizer.decode(\n",
    "  output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text: \", generated_text)\n",
    "\n",
    "# Load and compute the perplexity score\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(model_id='gpt2',\n",
    "                             predictions=generated_text)\n",
    "print(\"Perplexity: \", results['mean_perplexity'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generated Text:  Current trends show that by 2030, the number of people living in poverty will be at its lowest level\n",
    "Perplexity:  3441.6818263244627\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f464b53",
   "metadata": {},
   "source": [
    "### A feast of LLM metrics\n",
    "\n",
    "This iterative exercise will give you the chance of trying several metrics related to summarization, translation and question-answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc576ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rouge metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
    "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
    "\n",
    "# Calculate the rouge scores between the predicted and reference summaries\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE results: \", results)\n",
    "\n",
    "\"\"\"\n",
    "ROUGE results:  {'rouge1': 0.7719298245614034, 'rouge2': 0.6181818181818182, 'rougeL': 0.736842105263158, \n",
    "'rougeLsum': 0.736842105263158}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "llm_outputs = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
    "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
    "\n",
    "# Compute and print the METEOR score\n",
    "results = meteor.compute(predictions=llm_outputs, references=references)\n",
    "print(\"Meteor: \", results['meteor'])\n",
    "\n",
    "\"\"\"\n",
    "Meteor:  0.5350702240481536\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3240a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "# Compute the exact match and print the results\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(\"EM results: \", results)\n",
    "\n",
    "\"\"\"\n",
    "EM results:  {'exact_match': 0.3333333333333333}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71da6c3",
   "metadata": {},
   "source": [
    "### BLEU-proof translations\n",
    "\n",
    "Let's get familiar with the BLEU translation metric.\n",
    "\n",
    "A pipeline based on the Helsinki-NLP Spanish-English translation model and the BLEU metric has been loaded for you, using evaluate.load(\"bleu\") from the evaluate library.\n",
    "\n",
    "Given the following inputs and references for evaluation:\n",
    "\n",
    "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
    "\n",
    "reference_1 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
    "     ]\n",
    "\n",
    "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
    "\n",
    "references_2 = [\n",
    "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
    "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# Translate the first input sentence\n",
    "translated_output = translator(input_sentence_1)\n",
    "\n",
    "translated_sentence = translated_output[0]['translation_text']\n",
    "\n",
    "print(\"Translated:\", translated_sentence)\n",
    "\n",
    "# Calculate BLEU metric for translation quality\n",
    "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "Translated: Hey, how are you?\n",
    "{'bleu': 0.7598356856515925, 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666], 'brevity_penalty': 1.0, \n",
    "'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "translated_outputs = translator(input_sentences_2)\n",
    "\n",
    "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
    "print(predictions)\n",
    "\n",
    "results = bleu.compute(predictions=predictions, references=references_2)\n",
    "print(results)\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "    Translated: Hey, how are you?\n",
    "    {'bleu': 0.7598356856515925, 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666], 'brevity_penalty': 1.0, \n",
    "    'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n",
    "\n",
    "<script.py> output:\n",
    "    ['Hey, how are you?', \"I'm great, thanks.\"]\n",
    "    {'bleu': 0.8627788640890415, 'precisions': [0.9090909090909091, 0.8888888888888888, 0.8571428571428571, 0.8], \n",
    "    'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 11, 'reference_length': 11}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b465d79",
   "metadata": {},
   "source": [
    "## Model fine-tuning using human feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ef98d",
   "metadata": {},
   "source": [
    "### Setting up an RLHF loop\n",
    "\n",
    "The Proximal Policy Optimization (PPO) algorithm is popularly used in Reinforcement Learning from Human Feedback (RLHF) loops to fine-tune an LLM. The algorithm facilitates the iterative updating of model parameters based on a reward model derived from human feedback, ensuring the model's behavior is adapted predicated on human preferences.\n",
    "\n",
    "In this example, you will set up a simple RLHF loop based on PPO and a \"dummy\" reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "# Instantiate a reference model\n",
    "model_ref = create_reference_model(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Initialize trainer configuration\n",
    "ppo_config = PPOConfig(batch_size=1)\n",
    "\n",
    "prompt = \"Next year, I \"\n",
    "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "response  = respond_to_batch(model, input)\n",
    "\n",
    "# Create a PPOTrainer instance\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
    "reward = [torch.tensor(1.0)]\n",
    "\n",
    "# Train LLM for one step with PPO\n",
    "train_stats = ppo_trainer.step([input[0]], [response[0]], reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f8c57",
   "metadata": {},
   "source": [
    "## Challenges and ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de11758",
   "metadata": {},
   "source": [
    "### Toxic employee reviews?\n",
    "\n",
    "You have just joined a new company as a team lead. Two of your team members send thorough employee reviews on each other. To have a first, quick glimpse, you ask a pre-trained summarization LLM for help to get some concise points about each employee, as shown below:\n",
    "\n",
    "emp_1 = [\"Everyone in the team adores him\",\n",
    "           \"He is a true genius, pure talent\"]\n",
    "\n",
    "emp_2 = [\"Nobody in the team likes him\",\n",
    "           \"He is a useless 'good-for-nothing'\"]\n",
    "\n",
    "Your task is to carefully assess the toxicity level of these suggested responses.\n",
    "\n",
    "The necessary imports have been made for you, and the toxicity metric has been also loaded: toxicity_metric = load(\"toxicity\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
    "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
    "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
    "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
    "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])\n",
    "\n",
    "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation='maximum')\n",
    "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation='maximum')\n",
    "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
    "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])\n",
    "\n",
    "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation='ratio')\n",
    "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_1, aggregation='ratio')\n",
    "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
    "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])\n",
    "\n",
    "\"\"\"\n",
    "    Toxicities (emp. 1): [0.000138661649543792, 0.00013368616055231541]\n",
    "    Toxicities (emp. 2):  [0.00014245208876673132, 0.01007123477756977]\n",
    "    Maximum toxicity (emp. 1): 0.000138661649543792\n",
    "    Maximum toxicity (emp. 2):  0.01007123477756977\n",
    "    Toxicity ratio (emp. 1): 0.0\n",
    "    Toxicity ratio (emp. 2):  0.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8e1fb",
   "metadata": {},
   "source": [
    "### Best \"regard\"!\n",
    "\n",
    "In this wrap-up exercise, you will use the regard metric to assess polarity in two lists of sample texts associated to two groups of employees in two branches of a company. Each branch is located in a different country and geographical region of the planet.\n",
    "\n",
    "(It was reported a priori that due to naturally occurring cultural differences, both groups exhibit differences in the way they perform their work.)\n",
    "\n",
    "group1 = ['abc are described as loyal employees', \n",
    "          'abc are ambitious in their career expectations']\n",
    "\n",
    "group2 = ['abc are known for causing lots of team conflicts',\n",
    "          'abc are verbally violent']\n",
    "\n",
    "The evaluate library has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regard and regard-comparison metrics\n",
    "regard = evaluate.load('regard')\n",
    "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
    "\n",
    "# Compute the regard (polarities) of each group separately\n",
    "polarity_results_1 = regard.compute(data=group1)\n",
    "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
    "polarity_results_2 = regard.compute(data=group2)\n",
    "print(\"Polarity in group 2:\\n\", polarity_results_2)\n",
    "\n",
    "# Compute the relative regard between the two groups for comparison\n",
    "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
    "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Polarity in group 1:\n",
    "{'regard': [[{'label': 'positive', 'score': 0.9098385572433472}, {'label': 'neutral', 'score': 0.05939701199531555}, \n",
    "{'label': 'other', 'score': 0.026468148455023766}, {'label': 'negative', 'score': 0.0042962608858942986}], \n",
    "[{'label': 'positive', 'score': 0.7809811234474182}, {'label': 'neutral', 'score': 0.18085992336273193}, \n",
    "{'label': 'other', 'score': 0.030492939054965973}, {'label': 'negative', 'score': 0.007666011806577444}]]}\n",
    "\n",
    "Polarity in group 2:\n",
    "{'regard': [[{'label': 'negative', 'score': 0.9658734202384949}, {'label': 'other', 'score': 0.02155587635934353}, \n",
    "{'label': 'neutral', 'score': 0.012026473879814148}, {'label': 'positive', 'score': 0.0005441228277049959}], \n",
    "[{'label': 'negative', 'score': 0.9774737358093262}, {'label': 'other', 'score': 0.012994563207030296}, \n",
    "{'label': 'neutral', 'score': 0.008945498615503311}, {'label': 'positive', 'score': 0.0005862839752808213}]]}\n",
    "\n",
    "Polarity comparison between groups:\n",
    "{'regard_difference': {'positive': 0.8448446369438898, 'neutral': 0.10964248143136501, 'other': 0.011205323971807957, \n",
    "'negative': -0.9656924416776747}}\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
